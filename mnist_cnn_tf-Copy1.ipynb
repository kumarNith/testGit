{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kumar/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 1.5.0\n"
     ]
    }
   ],
   "source": [
    "print (\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = mnist_data.read_data_sets(\"data\", one_hot=True)#, reshape=True, validation_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "n_width   = 28\n",
    "n_height  = 28\n",
    "n_depth   = 1\n",
    "n_inputs = n_height * n_width * n_depth # total pixels\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(mnist.train.num_examples/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = tf.placeholder(tf.float32, [None,n_inputs])\n",
    "y = tf.placeholder(tf.float32,  [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.reshape(X_, shape = [-1, n_width, n_height, n_depth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = tf.placeholder(tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_size = 10 # No of classes\n",
    "mini_batch_size = 100\n",
    "epochs = 1000\n",
    "\n",
    "# three convolution layers \n",
    "K = 64   # First layer depth\n",
    "L = 128   # Second layer depth\n",
    "M = 256  # Third layer depth\n",
    "N = 400 # Fully connected \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W1 = tf.get_variable(\"W1\", shape=[5,5,1,K], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1 = tf.Variable(tf.ones([K])/10)\n",
    "W2 = tf.get_variable(\"W2\", shape=[5,5,K,L], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2 = tf.Variable(tf.ones([L])/10)\n",
    "W3 = tf.get_variable(\"W3\", shape=[4,4,L,M], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B3 = tf.Variable(tf.ones([M])/10)\n",
    "W4 = tf.get_variable(\"W4\", shape=[7,7,M,N], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B4 = tf.Variable(tf.ones([N])/10)\n",
    "W5 = tf.get_variable(\"W5\", shape=[N,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B5 = tf.Variable(tf.ones([class_size])/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.truncated_normal([5, 5, 1, K], stddev=0.1))  # 5x5 patch, 1 input channel, K output channels\n",
    "B1 = tf.Variable(tf.ones([K])/10)\n",
    "W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))\n",
    "B2 = tf.Variable(tf.ones([L])/10)\n",
    "W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))\n",
    "B3 = tf.Variable(tf.ones([M])/10)\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([7 * 7 * M, N], stddev=0.1))\n",
    "B4 = tf.Variable(tf.ones([N])/10)\n",
    "W5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))\n",
    "B5 = tf.Variable(tf.ones([10])/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 1 # stride\n",
    "y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1,s,s,1], padding = 'SAME') + B1)\n",
    "\n",
    "s = 2 # stride\n",
    "y2 = tf.nn.relu(tf.nn.conv2d(y1, W2, strides=[1,s,s,1], padding = 'SAME') + B2)\n",
    "\n",
    "s = 2 # stride\n",
    "y3 = tf.nn.relu(tf.nn.conv2d(y2, W3, strides=[1,s,s,1], padding = 'SAME') + B3)\n",
    "\n",
    "#falttern\n",
    "yy = tf.reshape(y3, shape=[-1, 7 * 7 * M])\n",
    "y4 = tf.nn.relu(tf.matmul(yy, W4) + B4)\n",
    "yLogit = tf.matmul(y4, W5) + B5\n",
    "\n",
    "prediction = tf.nn.softmax(yLogit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-68f353f8c79c>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=yLogit, labels=y)\n",
    "loss = tf.reduce_mean(entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001 +  tf.train.exponential_decay(0.003, step, 2000, 1/math.e)\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.09 loss: 28.616365\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a9463c2cfa62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX_\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\": accuracy:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" loss: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mlo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'acc' is not defined"
     ]
    }
   ],
   "source": [
    "a = 0.0\n",
    "c = 0.0\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in xrange(epochs):\n",
    "    batch_X, batch_y = mnist.train.next_batch(mini_batch_size)\n",
    "    a, c, _,_ = sess.run([accuracy, loss ,lr, optimizer], feed_dict={X_ : batch_X, y : batch_y, step : i})\n",
    "    print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c))\n",
    "    acc = a + acc\n",
    "    lo = c+lo\n",
    "print acc, lo\n",
    "print('Test accuracy is  ', acc/epochs)\n",
    "print('Test loss is  ', lo/epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0.0\n",
    "lo = 0.0\n",
    "for i in xrange(100):\n",
    "    batch_X, batch_y = mnist.test.next_batch(mini_batch_size)\n",
    "    a, c = sess.run([accuracy, loss],\n",
    "                                feed_dict={X_ : batch_X , y : batch_y })\n",
    "    print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "    acc = a + acc\n",
    "    lo = c+lo\n",
    "print acc, lo\n",
    "print('Test accuracy is  ', acc/100)\n",
    "print('Test loss is  ', lo/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
